{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ef9c9d-f6c9-43de-9f4f-8de74921d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports (esc + m to make cell into markdown)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from apyori import apriori #have to pip install apyori\n",
    "import math\n",
    "from mlxtend.frequent_patterns import fpmax #have to pip install mlxtend\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "import re\n",
    "\n",
    "#TODO\n",
    "#check support/purity measure: what is this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb7ed9-a8b6-4ee3-b7e5-a9cc40ebc563",
   "metadata": {},
   "source": [
    "Honor Code:  THIS CODE IS MY OWN WORK, IT WAS WRITTEN WITHOUT CONSULTING A  TUTOR OR CODE WRITTEN BY OTHER STUDENTS - David Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb10fb26-5ec6-4591-9335-1d2cb881dd23",
   "metadata": {},
   "source": [
    "Step 2: Pre Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00184244-fc0f-44c4-8e12-956ca4bcdd4d",
   "metadata": {},
   "source": [
    "1. Generate a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e972e8c3-e8a7-43ee-aaff-5da691f9f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a vocabulary (vocab.txt) from paper.txt\n",
    "\n",
    "# split the data with python\n",
    "\n",
    "paper_list = []\n",
    "\n",
    "with open(\"paper.txt\", 'r') as data_file:\n",
    "    for line in data_file:\n",
    "        data = line.split()\n",
    "        paper_list.append(data)\n",
    "\n",
    "# place all words into a dictionary called vocab.txt\n",
    "\n",
    "# build list first with all unique words\n",
    "unique_list = []\n",
    "\n",
    "for word_line in range(0, len(paper_list)):\n",
    "    for word in range(1, len(paper_list[word_line])): #start at 1 because 0 would be PaperID\n",
    "        # check if the word is unique or not\n",
    "        if paper_list[word_line][word] not in unique_list:\n",
    "            unique_list.append(paper_list[word_line][word])\n",
    "            \n",
    "# write the list into a text file\n",
    "with open(\"vocab.txt\", 'w') as data_file:\n",
    "    for word in unique_list:\n",
    "        data_file.write(word + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f978d55-e799-44b5-b0bb-72857df16398",
   "metadata": {},
   "source": [
    "2. Tokenize plain text by dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e190921-ca2c-440e-8198-6eaaf1aafb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transofrm each title (each line in paper.txt) into format of...\n",
    "# [M] (space) [t1]:[c1] (space) [t2]:[c2] (space) ...\n",
    "# [M] is number of unique terms in title\n",
    "# [count] is how many times the term appeared in title\n",
    "# name output title.txt\n",
    "\n",
    "vocab_list = []\n",
    "dict_key = []\n",
    "\n",
    "with open(\"vocab.txt\", 'r') as data_file:\n",
    "    for line in data_file:\n",
    "        data = line.split()\n",
    "        vocab_list.append(data)\n",
    "        \n",
    "for i in range(0, len(vocab_list)):\n",
    "    dict_key.append(i)\n",
    "\n",
    "tokenized = [] # to store the final list with tokenized counts\n",
    "\n",
    "\n",
    "\n",
    "for word_line in range(0, len(paper_list)):\n",
    "\n",
    "    words_in_line = []\n",
    "    words = paper_list[word_line]\n",
    "    for i in range(1, len(words)): # get rid of PaperID\n",
    "        words_in_line.append(words[i])\n",
    "        \n",
    "    # find the unique number of words in the line (M)\n",
    "    unique_list2 = []\n",
    "    \n",
    "    # build the unique list of words\n",
    "    for word in range(0, len(words_in_line)):\n",
    "        if words_in_line[word] not in unique_list2:\n",
    "            unique_list2.append(words_in_line[word])\n",
    "        \n",
    "                                              \n",
    "    M = len(unique_list2)\n",
    "    \n",
    "    for i in range(0, len(words_in_line)):\n",
    "        if M < len(words_in_line): #fix this bug where the index (len(unique_list2) is wrong when there are duplicates\n",
    "            M = M + 1\n",
    "    \n",
    "    #use word_in_line.count() to make a counts list, create list of indeces and create list of :\n",
    "    counts = []\n",
    "    indeces = []\n",
    "    colon = []\n",
    "    \n",
    "    #get index from vocab_list\n",
    "    for word in range(0, len(words_in_line)):\n",
    "        for vocab in range(0, len(vocab_list)):\n",
    "            vocab_word = \" \".join(vocab_list[vocab])\n",
    "            if (words_in_line[word] == vocab_word):\n",
    "                indeces.append(vocab) #vocab is the index of where the word is in the the vocabulary list\n",
    "        \n",
    "      \n",
    "        \n",
    "\n",
    "\n",
    "    #get counts\n",
    "    for count in range(0, len(words_in_line)):\n",
    "        counts.append(words_in_line.count(words_in_line[count]))\n",
    "        colon.append(':')\n",
    "        \n",
    "    \n",
    "    # equalize lengths\n",
    "    for i in range(0, len(words_in_line)):\n",
    "        if (len(counts) != len(indeces)):\n",
    "            indeces.append(0)\n",
    "   \n",
    "    \n",
    "        \n",
    "    #append everything created into sub_tokenized list by using a zip\n",
    "    sub_tokenized = []\n",
    "    for i in range(0, len(words_in_line)):\n",
    "        sub_tokenized.append(str(indeces[i]) + colon[i] + str(counts[i]))\n",
    "    \n",
    "    \n",
    "    # get rid of commas and brackets\n",
    "    sub_tokenized = \" \".join(map(str, sub_tokenized))\n",
    "        \n",
    "    #take method and apply to all lines in paper_list and add to tokenized\n",
    "    tokenized.append(str(M) + \" \" + str(sub_tokenized))\n",
    "    \n",
    "\n",
    "        \n",
    "                       \n",
    "        \n",
    "        \n",
    "# apply tokenized to a text file of title.txt\n",
    "\n",
    "with open(\"title.txt\", 'w') as data_file:\n",
    "    for token in tokenized:\n",
    "        data_file.write(token + \"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fbfc3-9438-425d-b3a6-84fe0ffe779f",
   "metadata": {},
   "source": [
    "Step 3: Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e0621-a37c-4bf3-bf78-2b16b181090f",
   "metadata": {},
   "source": [
    "1. Assign a topic to each term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30bd9c-b6e3-4df9-8981-8ce7174828b8",
   "metadata": {},
   "source": [
    "2. re-organize terms by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b961354c-f164-4ddc-baa1-4fb41efe61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using word-assignments.dat\n",
    "\n",
    "assign_list = []\n",
    "\n",
    "with open('word-assignments.dat', 'r') as data_file:\n",
    "    for line in data_file:\n",
    "        data = line.split()\n",
    "        assign_list.append(data)\n",
    "        \n",
    "# create the empty text files, made sure that topics range from 0-4 (5 topics)\n",
    "for i in range(0, 5):\n",
    "    with open('topic-' + str(i) + '.txt', 'w') as f:\n",
    "        pass\n",
    "    \n",
    "\n",
    "        \n",
    "for line in range(0, len(assign_list)):\n",
    "    \n",
    "    line0_topic = [] # to save individual lines of topics and IDs\n",
    "    line1_topic = []\n",
    "    line2_topic = []\n",
    "    line3_topic = []\n",
    "    line4_topic = []\n",
    "    \n",
    "    for term in range(1, len(assign_list[line])):\n",
    "        # topic always at index 6 and max is 5 topics (0, 1, 2, 3, 4)\n",
    "        ID = assign_list[line][term][0:4] # to get its ID \n",
    "        \n",
    "        \n",
    "        \n",
    "        if (assign_list[line][term][6] == '0'):\n",
    "            line0_topic.append(ID)\n",
    "        elif (assign_list[line][term][6] == '1'):\n",
    "            line1_topic.append(ID)\n",
    "        elif (assign_list[line][term][6] == '2'):\n",
    "            line2_topic.append(ID)\n",
    "        elif (assign_list[line][term][6] == '3'):\n",
    "            line3_topic.append(ID)\n",
    "        elif (assign_list[line][term][6] == '4'):\n",
    "            line4_topic.append(ID)\n",
    "                \n",
    "    \n",
    "    line0_topic = \" \".join(map(str, line0_topic))\n",
    "    line1_topic = \" \".join(map(str, line1_topic))\n",
    "    line2_topic = \" \".join(map(str, line2_topic))\n",
    "    line3_topic = \" \".join(map(str, line3_topic))\n",
    "    line4_topic = \" \".join(map(str, line4_topic))\n",
    "    \n",
    "    \n",
    "    for i in range(0, 5):\n",
    "        with open('topic-' + str(i) + '.txt', 'a') as f:\n",
    "            \n",
    "            #empty list has false condition so can use it to check if line is suppose to be empty\n",
    "            #effectively get rid of blank lines\n",
    "            if (i == 0):\n",
    "                if (line0_topic):\n",
    "                    f.write(str(line0_topic) + \"\\n\")\n",
    "            elif (i == 1):\n",
    "                if (line1_topic):\n",
    "                    f.write(str(line1_topic) + \"\\n\")\n",
    "            elif (i == 2):\n",
    "                if (line2_topic):\n",
    "                    f.write(str(line2_topic) + \"\\n\")\n",
    "            elif (i == 3):\n",
    "                if (line3_topic):\n",
    "                    f.write(str(line3_topic) + \"\\n\")\n",
    "            elif (i == 4): \n",
    "                if (line4_topic):\n",
    "                    f.write(str(line4_topic) + \"\\n\")\n",
    "                \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145df371-0cd0-4e9f-8d62-61f25cf83d31",
   "metadata": {},
   "source": [
    "Step 4: Mining Frequent Patterns for Each Topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9615575e-6536-4de5-8862-acf73bd3803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build method\n",
    "\n",
    "def frequent_pattern (inputfile, outputfile, min_support):\n",
    "    topic_list = []\n",
    "\n",
    "    with open(str(inputfile), 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = line.split()\n",
    "            topic_list.append(data)\n",
    "\n",
    "    # define the model\n",
    "    # get rid of support values\n",
    "    # for i in range(0, len(topic_list)):\n",
    "    #     topic_list[i].pop(0)\n",
    "\n",
    "    encoder = TransactionEncoder()\n",
    "    encoded_data = encoder.fit(topic_list).transform(topic_list)\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns = encoder.columns_)\n",
    "\n",
    "    # add to a list of items, make min support higher to get more valuable results\n",
    "    res = fpgrowth(encoded_df, min_support = min_support, use_colnames = True)\n",
    "\n",
    "    frequent_list =  res.values.tolist()\n",
    "\n",
    "    #sort the list and clean up appearence\n",
    "    support_vals = []\n",
    "    \n",
    "    # sort the support_vals\n",
    "\n",
    "    for i in range(0, len(frequent_list)):\n",
    "        support_vals.append(float(frequent_list[i][0]))\n",
    "        support_vals.sort(reverse = True)\n",
    "        \n",
    "    sort_list = []\n",
    "        \n",
    "    for i in range(0, len(support_vals)):\n",
    "        for j in range(0, len(frequent_list)):\n",
    "            if (support_vals[i] == frequent_list[j][0]):\n",
    "                sort_list.append(str(support_vals[i]) + \" \" + str(' '.join(list(frequent_list[j][1]))))\n",
    "                \n",
    "        sort_list[i].replace('[','').replace(']','').replace(',', '')\n",
    "                \n",
    "    \n",
    "\n",
    "    with open(\"patterns/\" + str(outputfile), 'w') as data_file:\n",
    "        for result in sort_list:\n",
    "            data_file.write(result + \"\\n\")\n",
    "            \n",
    "\n",
    "\n",
    "# decide min_support by percent of all transactions (Question to ponder A)\n",
    "# we are curious as to how interseting the rule/pattern is. Looking at the entire data and the supports,\n",
    "# the support value is ~0.01, so it would make sense to set that as the min_support to achieve the most patterns\n",
    "# while still having enough memory... By testing from high to low support values, a min_support of 0.01 makes the most sense in frequent patterns\n",
    "# Any min_supports below 0.01 would not be considered frequent patterns.\n",
    "\n",
    "\n",
    "for file in range(0, 5):\n",
    "    frequent_pattern(\"topic-\" + str(file) + \".txt\", \"pattern-\" + str(file) + \".txt\", min_support = 0.01)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f38b7-3034-479d-b27b-481bd3799751",
   "metadata": {},
   "source": [
    "Step 5: Mining Maximal/Closed Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19a16e08-0c63-4335-9d3e-a4bac5968bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_pattern(inputfile, outputfile):\n",
    "\n",
    "    topic_list = []\n",
    "\n",
    "    with open(\"patterns/\" + str(inputfile), 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = line.split()\n",
    "            topic_list.append(data)\n",
    "\n",
    "\n",
    "    # Mining closed patterns from scratch: frequent pattern where all super patterns are less frequent than closed pattern\n",
    "\n",
    "    # split up support and itemsets\n",
    "    supports_list = []\n",
    "    itemset_list = []\n",
    "    closed_patterns = []\n",
    "\n",
    "    for i in range(0, len(topic_list)):\n",
    "        supports_list.append(topic_list[i][0])\n",
    "        itemset_list.append(topic_list[i][1:])\n",
    "\n",
    "    for i in range(0, len(itemset_list)):\n",
    "        focal_itemset = itemset_list[i]\n",
    "        focal_support = supports_list[i]\n",
    "\n",
    "        for j in range(0, len(itemset_list)):\n",
    "            if (j == i):\n",
    "                continue\n",
    "\n",
    "            compare_itemset = itemset_list[j]\n",
    "            compare_support = supports_list[j]\n",
    "\n",
    "            if (len(focal_itemset) < len(compare_itemset)): #only checking subpatterns\n",
    "                presence = [k for e in focal_itemset for k in compare_itemset if e in k]\n",
    "                if (len(presence) > 0): # if focal is a subset of compare\n",
    "                    if (focal_support <= compare_support):\n",
    "                        break # if it already does not satisfy the criteria then go to next item\n",
    "                    else:\n",
    "                        closed_patterns.append(str(focal_support) + \" \" + str(focal_itemset))     \n",
    "\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # append biggest pattern (always satisfies closed patterns)\n",
    "    for i in range(0, len(itemset_list)):\n",
    "        freq = itemset_list[np.argmax([len(l) for l in itemset_list])]\n",
    "        if (itemset_list[i] == freq):\n",
    "            closed_patterns.append(str(supports_list[i]) + \" \" + str(itemset_list[i]))\n",
    "    \n",
    "    \n",
    "    # get rid of duplicates  \n",
    "    closed_patterns_f = []\n",
    "    for i in closed_patterns:\n",
    "        if i not in closed_patterns_f:\n",
    "            closed_patterns_f.append(i)\n",
    "\n",
    "    #configure to look nicer\n",
    "    for i in range(0, len(closed_patterns_f)):\n",
    "        closed_patterns_f[i] = closed_patterns_f[i].replace('[','').replace(']','').replace(',', '')\n",
    "        \n",
    "    with open(\"closed/\" + str(outputfile), 'w') as data_file:\n",
    "        for result in closed_patterns_f:\n",
    "            data_file.write(result + \"\\n\")\n",
    "            \n",
    "\n",
    "        \n",
    "for file in range(0, 5):\n",
    "    closed_pattern(\"pattern-\" + str(file) + \".txt\", \"closed-\" + str(file) + \".txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41bded5a-45a6-4254-a557-3cb3382adcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mining max patterns with FP max\n",
    "# max patterns: itemset X is max if X is frequent and there exists no frequent super-pattern with X\n",
    "\n",
    "def max_pattern(inputfile, outputfile, min_support):\n",
    "\n",
    "    topic_list = []\n",
    "\n",
    "    with open(str(inputfile), 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = line.split()\n",
    "            topic_list.append(data)\n",
    "\n",
    "    # define the model\n",
    "    # get rid of support values\n",
    "    # for i in range(0, len(topic_list)):\n",
    "    #     topic_list[i].pop(0)\n",
    "\n",
    "    encoder = TransactionEncoder()\n",
    "    encoded_data = encoder.fit(topic_list).transform(topic_list)\n",
    "    encoded_df = pd.DataFrame(encoded_data, columns = encoder.columns_)\n",
    "\n",
    "    # add to a list of items, make min support higher to get more valuable results\n",
    "    res = fpmax(encoded_df, min_support = min_support, use_colnames = True)\n",
    "\n",
    "    max_list =  res.values.tolist()\n",
    "\n",
    "    #sort the list and clean up appearence\n",
    "    \n",
    "    support_vals = []\n",
    "    \n",
    "    # sort the support_vals\n",
    "\n",
    "    for i in range(0, len(max_list)):\n",
    "        support_vals.append(float(max_list[i][0]))\n",
    "        support_vals.sort(reverse = True)\n",
    "        \n",
    "    sort_list = []\n",
    "        \n",
    "    for i in range(0, len(support_vals)):\n",
    "        for j in range(0, len(max_list)):\n",
    "            if (support_vals[i] == max_list[j][0]):\n",
    "                sort_list.append(str(support_vals[i]) + \" \" + str(' '.join(list(max_list[j][1]))))\n",
    "                \n",
    "        sort_list[i].replace('[','').replace(']','').replace(',', '')\n",
    "            \n",
    "            \n",
    "\n",
    "    with open(\"max/\" + str(outputfile), 'w') as data_file:\n",
    "        for result in sort_list:\n",
    "            data_file.write(result + \"\\n\")\n",
    "            \n",
    "for file in range(0, 5):\n",
    "    max_pattern(\"topic-\" + str(file) + \".txt\", \"max-\" + str(file) + \".txt\", 0.01) #similar minsupp as aprirori\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e397411-2757-4118-80de-23fc6e715876",
   "metadata": {},
   "source": [
    "Step 6: Re-rank by purity of patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "a3a97ab0-61cb-4092-82e1-0ff9bc082648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-rank patterns from support rankings\n",
    "\n",
    "    #f(t,p) is support of p in topic-t.txt\n",
    "    #D(t) is collection of documents where there is at least one word being assigned to topic t\n",
    "    #|D(t)| is exactly the number of lines in topic-t.txt\n",
    "    #D(t, t') is union of D(t) and D(t') that is the size of a set\n",
    "    #Note... |D(t,t') â‰  |D(t)| + |D(t')|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "09ea1bec-4ebe-4578-8069-add824b53f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_test(inputfile, outputfile):\n",
    "\n",
    "    pattern_support = []\n",
    "\n",
    "    with open(\"patterns/pattern-0.txt\", 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = line.split()\n",
    "            pattern_support.append(data)\n",
    "\n",
    "    # split up support and pattern\n",
    "    supports = [] #f(t,p)\n",
    "    patterns = [] #p\n",
    "\n",
    "    for item in range(0, len(pattern_support)):\n",
    "        supports.append(pattern_support[item][0])\n",
    "        patterns.append(pattern_support[item][1:])\n",
    "\n",
    "    lines = len(supports) #|D(t)|\n",
    "\n",
    "    #first part of equation\n",
    "\n",
    "    #calculate log(supports/total lines) for every line\n",
    "    first = []\n",
    "    for i in range(0, lines):\n",
    "        first.append(math.log(float(supports[i])/lines, 2)) #log based 2\n",
    "\n",
    "    #second part of equation\n",
    "    # use data from other topic pattern text files to compare the probability of seeing the pattern in other topics\n",
    "\n",
    "\n",
    "    # make list of all other supports and patterns found in other text files\n",
    "\n",
    "    other_supports = [] #f(t',p)\n",
    "    other_patterns = [] #p\n",
    "    for i in range(0, 5):\n",
    "\n",
    "        if i == 0: #if it is the current pattern text file being viewed\n",
    "            continue\n",
    "\n",
    "        other = []\n",
    "        with open('patterns/' + 'pattern-' + str(i) + '.txt', 'r') as data_file:\n",
    "            for line in data_file:\n",
    "                data = line.split()\n",
    "                other.append(data)\n",
    "\n",
    "\n",
    "\n",
    "        for item in range(0, len(other)):\n",
    "            other_supports.append(other[item][0])\n",
    "            other_patterns.append(other[item][1:])\n",
    "\n",
    "\n",
    "    # put the other lists and supports into a equation\n",
    "\n",
    "    #set union for D(t,t')\n",
    "    union = list(set().union(supports, other_supports))\n",
    "    other_lines = len(union)\n",
    "\n",
    "    # multiple lists for different patterns\n",
    "    multiple_patterns = {}\n",
    "    for i in range(0, len(patterns)):\n",
    "\n",
    "        #find max of numerator\n",
    "        multiple_patterns['l'+str(i)] = [] #now there are 5 lists for each type of pattern (using dictionary)\n",
    "        list_num = []\n",
    "\n",
    "        #build the dictionary of multiple patterns\n",
    "        for j in range(0, len(other_patterns)):\n",
    "            if (patterns[i] == other_patterns[j]):\n",
    "                num = (float(supports[i]) + float(other_supports[j]))/other_lines #numerator equation\n",
    "\n",
    "                multiple_patterns['l' + str(i)].append(num)\n",
    "\n",
    "    #find the max of each dictionary and put into new list called second \n",
    "    second = []\n",
    "    for i in range(0, len(multiple_patterns)):\n",
    "        if not multiple_patterns['l' + str(i)]:\n",
    "            max_num = 0.001 #equate as closest possible to 0 for empty sets that do not have other supports\n",
    "        else:\n",
    "            max_num = max(multiple_patterns['l' + str(i)])\n",
    "\n",
    "        second.append(math.log(max_num, 2))\n",
    "\n",
    "\n",
    "    #subtract all first from second to get purity for each pattern\n",
    "    purities = []\n",
    "\n",
    "    for i in range(0, len(patterns)):\n",
    "        purities.append(first[i] - second[i])\n",
    "\n",
    "    # add purities and support to the text list with the pattern as well\n",
    "    textoutput = []\n",
    "\n",
    "    for i in range(0, len(patterns)):\n",
    "        patterns[i] = (' '.join(patterns[i]))\n",
    "        textoutput.append(str(purities[i]) + \" \" + str(supports[i]) + \" \" + str(patterns[i]))\n",
    "\n",
    "\n",
    "    # convert to pandas to sort\n",
    "    df = pd.DataFrame(columns = (\"Purity\", \"Support\", \"Patterns\"))\n",
    "    df[\"Purity\"] = purities\n",
    "    df[\"Support\"] = supports\n",
    "    df[\"Patterns\"] = patterns\n",
    "\n",
    "\n",
    "    # noticed that the ones that appear more commonly have a higher purity so need to do subtraction from support - purity for sorting purposes\n",
    "    df[\"Purity\"] = df[\"Purity\"] * -1\n",
    "\n",
    "    df.sort_values(by = [\"Purity\", \"Support\"], ascending = False, inplace = True)\n",
    "    df.drop(\"Support\", axis = 1, inplace = True) # get rid of support after sorted\n",
    "\n",
    "    purity_results = df.values.tolist()\n",
    "\n",
    "    # write to a text file\n",
    "\n",
    "    with open(\"purity/\" + str(outputfile), 'w') as data_file:\n",
    "        for purity in purity_results:\n",
    "            data_file.write(str(purity) + \"\\n\")\n",
    "            \n",
    "            \n",
    "# run method on all files in pattern\n",
    "\n",
    "for file in range(0, 5):\n",
    "    purity_test(\"pattern-\" + str(file) + \".txt\", \"purity-\" + str(file) + \".txt\")\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f96057e-ea0d-459b-9579-003987c49d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_vocab(inputfile, outputfile):\n",
    "    \n",
    "    # correlate pattern with words from vocab_list\n",
    "\n",
    "    vocab_list = []\n",
    "\n",
    "    with open(\"vocab.txt\", 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = line.split()\n",
    "            vocab_list.append(data)\n",
    "\n",
    "    pattern_list = []\n",
    "\n",
    "    with open('patterns/' + str(inputfile) + '.txt', 'r') as data_file:\n",
    "        for line in data_file:\n",
    "            data = line.split()\n",
    "            pattern_list.append(data)\n",
    "\n",
    "    list_converted = []\n",
    "\n",
    "    for i in range(0, len(pattern_list)):\n",
    "        if (len(pattern_list[i]) == 2):\n",
    "            list_converted.append(vocab_list[int(pattern_list[i][1])])\n",
    "        else:\n",
    "            # only other case is when it has 2 words in one pattern\n",
    "            list_converted.append(vocab_list[int(pattern_list[i][1])] + vocab_list[int(pattern_list[i][2])])\n",
    "    \n",
    "    # change to text file\n",
    "    \n",
    "    \n",
    "    with open(\"patterns/\" + str(outputfile), 'w') as data_file:\n",
    "        for vocab in list_converted:\n",
    "            data_file.write(str(vocab) + \"\\n\")\n",
    "            \n",
    "    \n",
    "# run method on all files in pattern\n",
    "\n",
    "for file in range(0, 5):\n",
    "    convert_vocab(\"pattern-\" + str(file), \"pattern-\" + str(file) + \".txt.phrase\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd614157-7832-41de-ba5f-2875b515e66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
